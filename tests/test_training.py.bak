"""
Comprehensive tests for training module and core Forge functionality.

Covers:
- trainer.py: train_model() function
- forge.py: Forge class with training workflows
"""

import json
import os
import tempfile

import joblib
import numpy as np
import pandas as pd
import pytest
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from mlvern.core.forge import Forge
from mlvern.train.trainer import train_model
from mlvern.utils.registry import load_registry, save_registry

# ============================================================================
# FIXTURES
# ============================================================================


@pytest.fixture
def tmp_mlvern_dir():
    """Create a temporary directory for mlvern projects."""
    with tempfile.TemporaryDirectory() as tmpdir:
        yield tmpdir


@pytest.fixture
def sample_train_data():
    """Create sample training data with features and target."""
    np.random.seed(42)
    X = np.random.randn(100, 5)
    y = np.random.randint(0, 2, 100)
    return X, y


@pytest.fixture
def sample_val_data():
    """Create sample validation data."""
    np.random.seed(43)
    X = np.random.randn(30, 5)
    y = np.random.randint(0, 2, 30)
    return X, y


@pytest.fixture
def logistic_model():
    """Create a logistic regression model."""
    return LogisticRegression(random_state=42)


@pytest.fixture
def forest_model():
    """Create a random forest classifier."""
    return RandomForestClassifier(n_estimators=10, random_state=42)


@pytest.fixture
def sample_df():
    """Create sample dataframe with features and target."""
    np.random.seed(42)
    return pd.DataFrame(
        {
            "feat1": np.random.randn(50),
            "feat2": np.random.randn(50),
            "feat3": np.random.randn(50),
            "target": np.random.randint(0, 2, 50),
        }
    )


# ============================================================================
# TESTS: train_model()
# ============================================================================


class TestTrainModel:
    """Tests for the train_model() function."""

    def test_train_model_basic(self, sample_train_data, logistic_model):
        """Test basic training without validation data."""
        X, y = sample_train_data
        model, metrics = train_model(logistic_model, X, y)

        assert model is not None
        assert isinstance(metrics, dict)
        assert len(metrics) == 0  # No validation data provided

    def test_train_model_with_validation(
        self, sample_train_data, sample_val_data, logistic_model
    ):
        """Test training with validation data."""
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        model, metrics = train_model(
            logistic_model, X_train, y_train, X_val, y_val
        )

        assert model is not None
        assert "accuracy" in metrics
        assert 0 <= metrics["accuracy"] <= 1

    def test_train_model_accuracy_score(
        self, sample_train_data, sample_val_data, forest_model
    ):
        """Test that metrics contain valid accuracy scores."""
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        model, metrics = train_model(
            forest_model, X_train, y_train, X_val, y_val
        )
        preds = model.predict(X_val)
        expected_accuracy = accuracy_score(y_val, preds)

        assert metrics["accuracy"] == expected_accuracy

    def test_train_model_returns_fitted_model(
        self, sample_train_data, logistic_model
    ):
        """Test that returned model is fitted."""
        X, y = sample_train_data
        model, _ = train_model(logistic_model, X, y)

        # Fitted model should have coef_ attribute
        assert hasattr(model, "coef_")
        assert model.coef_ is not None

    def test_train_model_different_algorithms(
        self, sample_train_data, sample_val_data
    ):
        """Test training with different model algorithms."""
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        lr_model, lr_metrics = train_model(
            LogisticRegression(random_state=42),
            X_train,
            y_train,
            X_val,
            y_val,
        )
        rf_model, rf_metrics = train_model(
            RandomForestClassifier(n_estimators=5, random_state=42),
            X_train,
            y_train,
            X_val,
            y_val,
        )

        assert "accuracy" in lr_metrics
        assert "accuracy" in rf_metrics
        assert lr_model is not None
        assert rf_model is not None


# ============================================================================
# TESTS: Forge Class
# ============================================================================


class TestForgeInitialization:
    """Tests for Forge class initialization."""

    def test_forge_init_creates_project(self, tmp_mlvern_dir):
        """Test Forge initialization creates project structure."""
        forge = Forge("test_project", tmp_mlvern_dir)
        assert forge.project == "test_project"
        assert forge.base_dir == tmp_mlvern_dir
        assert forge.mlvern_dir == os.path.join(
            tmp_mlvern_dir, ".mlvern_test_project"
        )

    def test_forge_init_method(self, tmp_mlvern_dir):
        """Test Forge.init() creates required directories."""
        forge = Forge("myproject", tmp_mlvern_dir)
        forge.init()

        # Check directories are created
        assert os.path.exists(os.path.join(forge.mlvern_dir, "datasets"))
        assert os.path.exists(os.path.join(forge.mlvern_dir, "runs"))
        assert os.path.exists(os.path.join(forge.mlvern_dir, "models"))

    def test_forge_init_creates_registry(self, tmp_mlvern_dir):
        """Test Forge.init() creates registry.json."""
        forge = Forge("myproject", tmp_mlvern_dir)
        forge.init()

        registry_path = os.path.join(forge.mlvern_dir, "registry.json")
        assert os.path.exists(registry_path)

        registry = load_registry(forge.mlvern_dir)
        assert registry["project"] == "myproject"
        assert "created_at" in registry
        assert "datasets" in registry
        assert "runs" in registry

    def test_forge_multiple_init_idempotent(self, tmp_mlvern_dir):
        """Test that multiple init() calls don't cause errors."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()
        forge.init()  # Should not raise

        assert os.path.exists(os.path.join(forge.mlvern_dir, "datasets"))


class TestForgeDatasetOperations:
    """Tests for Forge dataset management."""

    def test_forge_register_dataset(self, tmp_mlvern_dir, sample_df):
        """Test registering a dataset."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, is_new = forge.register_dataset(sample_df, "target")

        assert is_new
        assert "dataset_hash" in fp
        assert fp["rows"] == 50
        assert fp["columns"] == 4
        assert fp["schema"]["target"] == "target"

    def test_forge_register_dataset_duplicate(self, tmp_mlvern_dir, sample_df):
        """Test registering the same dataset twice returns cached result."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp1, is_new1 = forge.register_dataset(sample_df, "target")
        fp2, is_new2 = forge.register_dataset(sample_df, "target")

        assert is_new1  # First registration
        assert not is_new2  # Second is from cache
        assert fp1["dataset_hash"] == fp2["dataset_hash"]

    def test_forge_list_datasets(self, tmp_mlvern_dir, sample_df):
        """Test listing registered datasets."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        datasets = forge.list_datasets()
        assert len(datasets) == 0

        fp, _ = forge.register_dataset(sample_df, "target")
        datasets = forge.list_datasets()

        assert len(datasets) == 1
        assert fp["dataset_hash"] in datasets

    def test_forge_list_datasets_multiple(self, tmp_mlvern_dir):
        """Test listing multiple datasets."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        df1 = pd.DataFrame({"a": [1, 2, 3], "target": [0, 1, 0]})
        df2 = pd.DataFrame({
            "b": [4, 5, 6],
            "c": [7, 8, 9],
            "target": [1, 0, 1]
        })

        fp1, _ = forge.register_dataset(df1, "target")
        fp2, _ = forge.register_dataset(df2, "target")

        datasets = forge.list_datasets()
        assert len(datasets) == 2
        assert fp1["dataset_hash"] in datasets
        assert fp2["dataset_hash"] in datasets


class TestForgeTrainingRun:
    """Tests for Forge.run() training workflow."""

    def test_forge_run_basic(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test basic training run workflow."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        # Register dataset
        fp, _ = forge.register_dataset(sample_df, "target")

        # Prepare data
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data
        config = {"model": "logistic", "learning_rate": 0.01}

        # Run training
        run_id, metrics = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, config, fp
        )

        assert run_id is not None
        assert run_id.startswith("run_")
        assert "accuracy" in metrics
        assert 0 <= metrics["accuracy"] <= 1

    def test_forge_run_creates_run_directory(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run creates proper directory structure."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data
        config = {"model": "lr"}

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, config, fp
        )

        run_path = os.path.join(forge.mlvern_dir, "runs", run_id)
        assert os.path.exists(run_path)
        assert os.path.exists(os.path.join(run_path, "meta.json"))
        assert os.path.exists(os.path.join(run_path, "config.json"))
        assert os.path.exists(os.path.join(run_path, "metrics.json"))

    def test_forge_run_saves_model(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run saves model artifact."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )
        model_path = os.path.join(
            forge.mlvern_dir, "runs", run_id, "artifacts", "model.pkl"
        )
        assert os.path.exists(model_path)

        # Load and verify model
        loaded_model = joblib.load(model_path)
        assert loaded_model is not None

    def test_forge_run_saves_metadata(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run saves metadata correctly."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        meta_path = os.path.join(forge.mlvern_dir, "runs", run_id, "meta.json")
        with open(meta_path) as f:
            meta = json.load(f)

        assert meta["run_id"] == run_id
        assert meta["dataset_hash"] == fp["dataset_hash"]
        assert "timestamp" in meta

    def test_forge_run_saves_config(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run saves config correctly."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data
        config = {"model": "logistic", "epochs": 50, "batch_size": 32}

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, config, fp
        )

        config_path = os.path.join(
            forge.mlvern_dir, "runs", run_id, "config.json"
        )
        with open(config_path) as f:
            saved_config = json.load(f)

        assert saved_config == config

    def test_forge_run_saves_metrics(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run saves metrics correctly."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, metrics = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        metrics_path = os.path.join(
            forge.mlvern_dir, "runs", run_id, "metrics.json"
        )
        with open(metrics_path) as f:
            saved_metrics = json.load(f)

        assert "accuracy" in saved_metrics
        assert saved_metrics["accuracy"] == metrics["accuracy"]

    def test_forge_list_runs(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test listing all runs."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        runs = forge.list_runs()
        assert len(runs) == 0

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        runs = forge.list_runs()
        assert len(runs) == 1
        assert run_id in runs

    def test_forge_multiple_runs(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
        forest_model,
    ):
        """Test tracking multiple runs."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id1, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )
        run_id2, _ = forge.run(
            forest_model, X_train, y_train, X_val, y_val, {}, fp
        )

        runs = forge.list_runs()
        assert len(runs) == 2
        assert run_id1 in runs
        assert run_id2 in runs

    def test_forge_run_registry_update(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run updates registry correctly."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, metrics = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        registry = load_registry(forge.mlvern_dir)
        assert run_id in registry["runs"]
        assert registry["runs"][run_id]["model"] == "LogisticRegression"
        assert registry["runs"][run_id]["metrics"] == metrics


class TestForgeIntegration:
    """Integration tests for complete Forge workflows."""

    def test_forge_complete_workflow(
        self, tmp_mlvern_dir, sample_df, sample_train_data, sample_val_data
    ):
        """Test complete workflow: init -> register -> train -> list."""
        forge = Forge("ml_project", tmp_mlvern_dir)
        forge.init()

        # Register dataset
        fp, is_new = forge.register_dataset(sample_df, "target")
        assert is_new

        # List datasets
        datasets = forge.list_datasets()
        assert len(datasets) == 1

        # Train models
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id1, _ = forge.run(
            LogisticRegression(random_state=42),
            X_train,
            y_train,
            X_val,
            y_val,
            {"model": "logistic"},
            fp,
        )
        run_id2, _ = forge.run(
            RandomForestClassifier(n_estimators=5, random_state=42),
            X_train,
            y_train,
            X_val,
            y_val,
            {"model": "rf"},
            fp,
        )

        # List runs
        runs = forge.list_runs()
        assert len(runs) == 2
        assert run_id1 in runs
        assert run_id2 in runs

    def test_forge_different_projects(self, tmp_mlvern_dir, sample_df):
        """Test managing multiple projects in same directory."""
        forge1 = Forge("project_a", tmp_mlvern_dir)
        forge2 = Forge("project_b", tmp_mlvern_dir)

        forge1.init()
        forge2.init()

        fp1, _ = forge1.register_dataset(sample_df, "target")
        fp2, _ = forge2.register_dataset(sample_df, "target")

        # Verify they're in different registries
        registry1 = load_registry(forge1.mlvern_dir)
        registry2 = load_registry(forge2.mlvern_dir)

        assert registry1["project"] == "project_a"
        assert registry2["project"] == "project_b"
        assert fp1["dataset_hash"] in registry1["datasets"]
        assert fp2["dataset_hash"] in registry2["datasets"]


class TestRegistrySaveOperations:
    """Tests for saving registry state during Forge operations."""

    def test_forge_run_persists_registry(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that run updates are persisted in registry file."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        # Reload registry from disk to verify persistence
        registry = load_registry(forge.mlvern_dir)

        assert run_id in registry["runs"]
        assert registry["runs"][run_id]["dataset_hash"] == fp["dataset_hash"]
        assert registry["runs"][run_id]["model"] == "LogisticRegression"

    def test_manual_registry_save_load_roundtrip(self, tmp_mlvern_dir):
        """Test manual save/load of registry with custom data."""
        custom_registry = {
            "project": "test",
            "datasets": {
                "hash123": {
                    "rows": 100,
                    "columns": 5,
                    "target": "label",
                }
            },
            "runs": {
                "run_001": {
                    "model": "RandomForest",
                    "accuracy": 0.95,
                }
            },
            "metadata": {
                "version": "1.0",
                "author": "test_user",
            },
        }

        # Save registry
        save_registry(tmp_mlvern_dir, custom_registry)

        # Load and verify
        loaded = load_registry(tmp_mlvern_dir)

        assert loaded == custom_registry
        assert loaded["metadata"]["version"] == "1.0"
        assert loaded["datasets"]["hash123"]["rows"] == 100
        assert loaded["runs"]["run_001"]["accuracy"] == 0.95


# ============================================================================
# TESTS: New Forge API - Dataset Accessors
# ============================================================================


class TestForgeDatasetAccessors:
    """Tests for Forge dataset accessor methods."""

    def test_get_dataset_path_success(self, tmp_mlvern_dir, sample_df):
        """Test retrieving dataset path by hash."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        path = forge.get_dataset_path(dataset_hash)

        assert path is not None
        assert os.path.exists(path)
        assert dataset_hash in path

    def test_get_dataset_path_not_found(self, tmp_mlvern_dir):
        """Test get_dataset_path raises error for non-existent hash."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        with pytest.raises(ValueError, match="not found"):
            forge.get_dataset_path("nonexistent_hash")

    def test_load_dataset_metadata(self, tmp_mlvern_dir, sample_df):
        """Test loading dataset metadata and paths."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        dataset_info = forge.load_dataset(dataset_hash)

        assert dataset_info["dataset_hash"] == dataset_hash
        assert "path" in dataset_info
        assert "schema" in dataset_info
        assert "metadata" in dataset_info
        assert "report_paths" in dataset_info
        assert "plot_paths" in dataset_info
        assert dataset_info["schema"]["target"] == "target"

    def test_load_dataset_not_found(self, tmp_mlvern_dir):
        """Test load_dataset raises error for non-existent hash."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        with pytest.raises(ValueError):
            forge.load_dataset("nonexistent_hash")

    def test_load_dataset_has_reports(self, tmp_mlvern_dir, sample_df):
        """Test that loaded dataset includes report paths when available."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        dataset_info = forge.load_dataset(dataset_hash)

        assert "report_paths" in dataset_info
        assert isinstance(dataset_info["report_paths"], dict)


# ============================================================================
# TESTS: New Forge API - Run/Model Accessors
# ============================================================================


class TestForgeRunAccessors:
    """Tests for Forge run and model accessor methods."""

    def test_get_run_success(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test retrieving run metadata and information."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        run_info = forge.get_run(run_id)

        assert run_info["run_id"] == run_id
        assert "metadata" in run_info
        assert "metrics" in run_info
        assert "config" in run_info
        assert "tags" in run_info
        assert "registry_info" in run_info

    def test_get_run_not_found(self, tmp_mlvern_dir):
        """Test get_run raises error for non-existent run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        with pytest.raises(ValueError, match="not found"):
            forge.get_run("nonexistent_run")

    def test_get_run_metrics(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test retrieving metrics for a specific run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, metrics = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        retrieved_metrics = forge.get_run_metrics(run_id)

        assert retrieved_metrics == metrics
        assert "accuracy" in retrieved_metrics

    def test_get_run_artifacts(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test retrieving artifact paths for a run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        artifacts = forge.get_run_artifacts(run_id)

        assert "run_dir" in artifacts
        assert "meta.json" in artifacts
        assert "config.json" in artifacts
        assert "metrics.json" in artifacts
        assert "artifact_model.pkl" in artifacts
        assert all(os.path.exists(path) for path in artifacts.values())

    def test_load_model_success(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test loading a trained model from a run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train,
            X_val, y_val, {}, fp
        )

        loaded_model = forge.load_model(run_id)

        assert loaded_model is not None
        assert hasattr(loaded_model, "predict")
        assert hasattr(loaded_model, "coef_")

    def test_load_model_not_found(self, tmp_mlvern_dir):
        """Test load_model raises error for non-existent run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        with pytest.raises(ValueError):
            forge.load_model("nonexistent_run")

    def test_load_model_makes_predictions(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that loaded model can make predictions."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        loaded_model = forge.load_model(run_id)
        predictions = loaded_model.predict(X_val)

        assert predictions is not None
        assert len(predictions) == len(X_val)


# ============================================================================
# TESTS: New Forge API - Model Registry
# ============================================================================


class TestForgeModelRegistry:
    """Tests for Forge model registry methods."""

    def test_register_model_success(self, tmp_mlvern_dir, logistic_model):
        """Test registering a model."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        metadata = {
            "model_name": "logistic_regression_v1",
            "description": "Test model",
            "hyperparameters": {"C": 1.0},
        }

        model_id = forge.register_model(logistic_model, metadata)

        assert model_id is not None
        assert "model_" in model_id

        # Verify model was saved
        model_path = os.path.join(
            forge.mlvern_dir, "models", f"{model_id}.pkl"
        )
        assert os.path.exists(model_path)

    def test_register_model_custom_id(self, tmp_mlvern_dir, logistic_model):
        """Test registering a model with custom ID."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        metadata = {"model_name": "test_model"}
        custom_id = "custom_model_v1"

        model_id = forge.register_model(
            logistic_model, metadata, model_id=custom_id
        )

        assert model_id == custom_id

    def test_list_models(self, tmp_mlvern_dir, logistic_model, forest_model):
        """Test listing registered models."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        models = forge.list_models()
        assert len(models) == 0

        id1 = forge.register_model(
            logistic_model, {"name": "lr"}, model_id="model_lr_v1"
        )
        id2 = forge.register_model(
            forest_model, {"name": "rf"}, model_id="model_rf_v1"
        )

        models = forge.list_models()
        assert len(models) == 2
        assert id1 in models
        assert id2 in models

    def test_register_model_metadata_saved(
        self, tmp_mlvern_dir, logistic_model
    ):
        """Test that model metadata is saved alongside the model."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        metadata = {
            "model_name": "test_model",
            "version": "1.0",
            "source_run_id": "run_123",
        }

        model_id = forge.register_model(logistic_model, metadata)

        model_path = os.path.join(
            forge.mlvern_dir, "models", f"{model_id}.pkl"
        )
        metadata_path = model_path.replace(".pkl", "_metadata.json")

        assert os.path.exists(metadata_path)

        with open(metadata_path) as f:
            saved_metadata = json.load(f)

        assert saved_metadata["model_name"] == "test_model"
        assert saved_metadata["version"] == "1.0"

    def test_tag_run_success(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test tagging a run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        tags = {"experiment": "baseline", "status": "approved"}
        forge.tag_run(run_id, tags)

        retrieved_tags = forge.get_run_tags(run_id)

        assert retrieved_tags["experiment"] == "baseline"
        assert retrieved_tags["status"] == "approved"

    def test_tag_run_merge_tags(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that tagging merges with existing tags."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        forge.tag_run(run_id, {"tag1": "value1"})
        forge.tag_run(run_id, {"tag2": "value2"})

        tags = forge.get_run_tags(run_id)

        assert tags["tag1"] == "value1"
        assert tags["tag2"] == "value2"

    def test_get_run_tags_nonexistent_run(self, tmp_mlvern_dir):
        """Test getting tags for non-existent run returns empty dict."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        tags = forge.get_run_tags("nonexistent_run")

        assert tags == {}


# ============================================================================
# TESTS: New Forge API - Deletion & Cleanup
# ============================================================================


class TestForgeCleanup:
    """Tests for Forge cleanup and deletion methods."""

    def test_remove_run_requires_confirmation(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test remove_run requires confirm=True."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        result = forge.remove_run(run_id, confirm=False)

        assert result is False

        # Verify run still exists
        assert run_id in forge.list_runs()

    def test_remove_run_success(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test removing a run with confirmation."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        result = forge.remove_run(run_id, confirm=True)

        assert result is True
        assert run_id not in forge.list_runs()

        run_path = os.path.join(forge.mlvern_dir, "runs", run_id)
        assert not os.path.exists(run_path)

    def test_remove_run_not_found(self, tmp_mlvern_dir):
        """Test remove_run handles non-existent run."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        result = forge.remove_run("nonexistent_run", confirm=True)

        assert result is False

    def test_prune_datasets_requires_confirmation(
        self, tmp_mlvern_dir, sample_df
    ):
        """Test prune_datasets requires confirm=True."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        result = forge.prune_datasets(older_than_days=0, confirm=False)

        assert result == []
        assert dataset_hash in forge.list_datasets()

    def test_prune_datasets_no_old_datasets(self, tmp_mlvern_dir, sample_df):
        """Test prune_datasets when no datasets are old enough."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        result = forge.prune_datasets(older_than_days=365, confirm=True)

        assert result == []
        assert dataset_hash in forge.list_datasets()

    def test_get_project_stats(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test getting project statistics."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        forge.run(logistic_model, X_train, y_train, X_val, y_val, {}, fp)

        forge.register_model(logistic_model, {"name": "test"})

        stats = forge.get_project_stats()

        assert stats["project"] == "project"
        assert stats["datasets_count"] == 1
        assert stats["runs_count"] == 1
        assert stats["models_count"] == 1
        assert "total_size_mb" in stats


# ============================================================================
# TESTS: New Forge API - Evaluation & Prediction
# ============================================================================


class TestForgeEvaluation:
    """Tests for Forge evaluation and prediction methods."""

    def test_predict_with_run_id(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test prediction using run_id."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        predictions = forge.predict(run_id, X_val)

        assert predictions is not None
        assert len(predictions) == len(X_val)

    def test_predict_with_model_object(
        self, sample_train_data, sample_val_data, logistic_model
    ):
        """Test prediction using model object directly."""
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        logistic_model.fit(X_train, y_train)
        forge = Forge("project")

        predictions = forge.predict(logistic_model, X_val)

        assert predictions is not None
        assert len(predictions) == len(X_val)

    def test_evaluate_with_run_id(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test evaluation using run_id."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(logistic_model, X_train,
                              y_train, X_val, y_val, {}, fp)

        result = forge.evaluate(run_id, X_val, y_val)

        assert "metrics" in result
        assert "accuracy" in result["metrics"]
        assert "precision" in result["metrics"]
        assert "recall" in result["metrics"]
        assert "f1" in result["metrics"]

    def test_evaluate_with_model_object(
        self, sample_train_data, sample_val_data, logistic_model, tmp_path
    ):
        """Test evaluation using model object directly."""
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        logistic_model.fit(X_train, y_train)
        forge = Forge("project")

        output_dir = os.path.join(str(tmp_path), "evaluation")
        result = forge.evaluate(
            logistic_model, X_val, y_val, output_dir=output_dir
        )

        assert "metrics" in result
        assert "accuracy" in result["metrics"]
        assert isinstance(result["metrics"]["accuracy"], float)

    def test_evaluate_saves_report(
        self,
        tmp_mlvern_dir,
        sample_df,
        sample_train_data,
        sample_val_data,
        logistic_model,
    ):
        """Test that evaluation saves a report file."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        X_train, y_train = sample_train_data
        X_val, y_val = sample_val_data

        run_id, _ = forge.run(
            logistic_model, X_train, y_train, X_val, y_val, {}, fp
        )

        forge.evaluate(run_id, X_val, y_val)

        eval_report_path = os.path.join(
            forge.mlvern_dir, "runs", run_id, "evaluation",
            "evaluation_report.json"
        )
        assert os.path.exists(eval_report_path)

        with open(eval_report_path) as f:
            report = json.load(f)

        assert "metrics" in report


# ============================================================================
# TESTS: New Forge API - Dataset Saving & Loading
# ============================================================================


class TestForgeDatasetSaveLoad:
    """Tests for Forge dataset save/load methods."""

    def test_save_dataset(self, tmp_mlvern_dir, sample_df):
        """Test saving a dataset."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        result = forge.save_dataset(
            sample_df, dataset_hash, name="test_dataset"
        )

        assert result["dataset_hash"] == dataset_hash
        assert result["saved"] is True
        assert result["metadata"]["name"] == "test_dataset"

    def test_save_dataset_with_tags(self, tmp_mlvern_dir, sample_df):
        """Test saving dataset with tags."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        tags = {"experiment": "exp1", "version": "v1"}
        result = forge.save_dataset(sample_df, dataset_hash, tags=tags)

        assert result["metadata"]["tags"] == tags

    def test_load_dataset_by_hash(self, tmp_mlvern_dir, sample_df):
        """Test loading dataset by hash."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        forge.save_dataset(sample_df, dataset_hash)

        loaded_df = forge.load_dataset_by_hash(dataset_hash)

        assert loaded_df.shape == sample_df.shape
        assert list(loaded_df.columns) == list(sample_df.columns)

    def test_load_dataset_by_hash_not_found(self, tmp_mlvern_dir):
        """Test loading non-existent dataset raises error."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        with pytest.raises(FileNotFoundError):
            forge.load_dataset_by_hash("nonexistent_hash")

    def test_get_dataset_report(self, tmp_mlvern_dir, sample_df):
        """Test getting aggregated dataset report."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        fp, _ = forge.register_dataset(sample_df, "target")
        dataset_hash = fp["dataset_hash"]

        report = forge.get_dataset_report(dataset_hash)

        assert "dataset_hash" in report
        assert report["dataset_hash"] == dataset_hash
        assert "metadata" in report
        assert "inspection" in report
        assert "statistics" in report
        assert "risk" in report

    def test_load_dataset_preserves_dtypes(self, tmp_mlvern_dir):
        """Test that loading preserves data types."""
        forge = Forge("project", tmp_mlvern_dir)
        forge.init()

        df = pd.DataFrame(
            {
                "int_col": [1, 2, 3],
                "float_col": [1.1, 2.2, 3.3],
                "str_col": ["a", "b", "c"],
                "target": [0, 1, 0],
            }
        )

        fp, _ = forge.register_dataset(df, "target")
        dataset_hash = fp["dataset_hash"]

        forge.save_dataset(df, dataset_hash)
        loaded_df = forge.load_dataset_by_hash(dataset_hash)

        assert loaded_df["int_col"].dtype == df["int_col"].dtype
        assert loaded_df["float_col"].dtype == df["float_col"].dtype
        assert loaded_df["str_col"].dtype == df["str_col"].dtype
